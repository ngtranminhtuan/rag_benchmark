{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 621/621 [00:00<00:00, 8.80MB/s]\n",
      "Downloading data: 100%|██████████| 1.15M/1.15M [00:01<00:00, 587kB/s]\n",
      "Generating train split: 100%|██████████| 232/232 [00:00<00:00, 51282.14 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'correct_answer', 'incorrect_answer', 'question_id', 'generated_with_rag', 'context', 'generated_without_rag'],\n",
       "        num_rows: 232\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "datasets = load_dataset('explodinggradients/ragas-wikiqa')\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = datasets['train']['question']\n",
    "correct_answers = datasets['train']['correct_answer']\n",
    "incorrect_answers = datasets['train']['incorrect_answer']\n",
    "contexts = datasets['train']['context']\n",
    "\n",
    "data = {\n",
    "    'question': questions,\n",
    "    'answer': correct_answers,\n",
    "    'contexts': contexts,\n",
    "    'ground_truth': correct_answers  # Using correct answers as ground truth\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'contexts', 'ground_truth'],\n",
       "    num_rows: 232\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_correctness\n",
    "\n",
    "score = evaluate(data,metrics=[faithfulness,answer_correctness])\n",
    "score.to_pandas().to_csv('res.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/weaviate/warnings.py:158: DeprecationWarning: Dep016: You are using the Weaviate v3 client, which is deprecated.\n",
      "            Consider upgrading to the new and improved v4 client instead!\n",
      "            See here for usage: https://weaviate.io/developers/weaviate/client-libraries/python\n",
      "            \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Weaviate\n",
    "\n",
    "# Thiết lập API key của Weaviate\n",
    "os.environ[\"WEAVIATE_API_KEY\"] = '123456'\n",
    "WEAVIATE_API_KEY = os.environ[\"WEAVIATE_API_KEY\"]\n",
    "\n",
    "# Load dataset\n",
    "datasets = load_dataset('explodinggradients/ragas-wikiqa')\n",
    "contexts = datasets['train']['context']\n",
    "\n",
    "# Embed context\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Initialize Weaviate client\n",
    "WEAVIATE_URL='http://localhost:8080'\n",
    "\n",
    "# Store embedded contexts in Weaviate\n",
    "for i, context_list in enumerate(contexts):\n",
    "    for j, context in enumerate(context_list):\n",
    "        # Store each context in Weaviate\n",
    "        docsearch = Weaviate.from_texts(\n",
    "            [context],\n",
    "            embeddings,\n",
    "            weaviate_url=WEAVIATE_URL,\n",
    "            by_text=False,\n",
    "            metadatas=[{\"source\": f\"{i}-pl-{j}\"}],\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  84%|████████▎ | 388/464 [02:11<00:36,  2.07it/s]Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/ragas/executor.py\", line 79, in _aresults\n",
      "    r = await future\n",
      "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/ragas/executor.py\", line 38, in sema_coro\n",
      "    return await coro\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/ragas/executor.py\", line 112, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/ragas/metrics/base.py\", line 93, in ascore\n",
      "    raise e\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/ragas/metrics/base.py\", line 89, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/ragas/metrics/_faithfulness.py\", line 186, in _ascore\n",
      "    nli_result = await self.llm.generate(\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/ragas/llms/base.py\", line 110, in generate\n",
      "    return await loop.run_in_executor(None, generate_text)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/tenacity/__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/tenacity/__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/tenacity/__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/tenacity/__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/tenacity/__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/ragas/llms/base.py\", line 139, in generate_text\n",
      "    return self.langchain_llm.generate_prompt(\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 546, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 407, in generate\n",
      "    raise e\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 397, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 589, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 484, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 667, in create\n",
      "    return self._post(\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/openai/_base_client.py\", line 1208, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/openai/_base_client.py\", line 897, in request\n",
      "    return self._request(\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/openai/_base_client.py\", line 973, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/openai/_base_client.py\", line 1021, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/openai/_base_client.py\", line 973, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/openai/_base_client.py\", line 1021, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/openai/_base_client.py\", line 988, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-16k in organization org-hMujw7VtKfmhsEST1EsSIINi on tokens per min (TPM): Limit 60000, Used 59305, Requested 2477. Please try again in 1.782s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Evaluating:  95%|█████████▍| 440/464 [02:37<00:18,  1.32it/s]Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/ragas/executor.py\", line 79, in _aresults\n",
      "    r = await future\n",
      "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/ragas/executor.py\", line 38, in sema_coro\n",
      "    return await coro\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/ragas/executor.py\", line 112, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/ragas/metrics/base.py\", line 93, in ascore\n",
      "    raise e\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/ragas/metrics/base.py\", line 89, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/ragas/metrics/_faithfulness.py\", line 186, in _ascore\n",
      "    nli_result = await self.llm.generate(\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/ragas/llms/base.py\", line 110, in generate\n",
      "    return await loop.run_in_executor(None, generate_text)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/tenacity/__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/tenacity/__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/tenacity/__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/tenacity/__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/tenacity/__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/ragas/llms/base.py\", line 139, in generate_text\n",
      "    return self.langchain_llm.generate_prompt(\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 546, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 407, in generate\n",
      "    raise e\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 397, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 589, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 484, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 667, in create\n",
      "    return self._post(\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/openai/_base_client.py\", line 1208, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/openai/_base_client.py\", line 897, in request\n",
      "    return self._request(\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/openai/_base_client.py\", line 973, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/openai/_base_client.py\", line 1021, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/openai/_base_client.py\", line 973, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/openai/_base_client.py\", line 1021, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/openai/_base_client.py\", line 988, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-16k in organization org-hMujw7VtKfmhsEST1EsSIINi on tokens per min (TPM): Limit 60000, Used 59310, Requested 2601. Please try again in 1.911s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Evaluating:  97%|█████████▋| 450/464 [02:43<00:09,  1.48it/s]Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/ragas/executor.py\", line 79, in _aresults\n",
      "    r = await future\n",
      "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/ragas/executor.py\", line 38, in sema_coro\n",
      "    return await coro\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/ragas/executor.py\", line 112, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/ragas/metrics/base.py\", line 93, in ascore\n",
      "    raise e\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/ragas/metrics/base.py\", line 89, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/ragas/metrics/_faithfulness.py\", line 186, in _ascore\n",
      "    nli_result = await self.llm.generate(\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/ragas/llms/base.py\", line 110, in generate\n",
      "    return await loop.run_in_executor(None, generate_text)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/tenacity/__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/tenacity/__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/tenacity/__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/tenacity/__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/tenacity/__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/ragas/llms/base.py\", line 139, in generate_text\n",
      "    return self.langchain_llm.generate_prompt(\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 546, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 407, in generate\n",
      "    raise e\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 397, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 589, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 484, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 667, in create\n",
      "    return self._post(\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/openai/_base_client.py\", line 1208, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/openai/_base_client.py\", line 897, in request\n",
      "    return self._request(\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/openai/_base_client.py\", line 973, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/openai/_base_client.py\", line 1021, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/openai/_base_client.py\", line 973, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/openai/_base_client.py\", line 1021, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/home/tuan/Downloads/rag_flow/env/lib/python3.10/site-packages/openai/_base_client.py\", line 988, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-16k in organization org-hMujw7VtKfmhsEST1EsSIINi on tokens per min (TPM): Limit 60000, Used 59692, Requested 2461. Please try again in 2.153s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Evaluating: 100%|██████████| 464/464 [03:46<00:00,  2.05it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def generate_answers(question):\n",
    "    # Set up RAG pipeline\n",
    "    retriever = docsearch.as_retriever()\n",
    "    template = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "    Question: {question} \n",
    "    Context: {context} \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "    output_parser = StrOutputParser()\n",
    "    \n",
    "    # Build RAG pipeline\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | output_parser\n",
    "    )\n",
    "    \n",
    "    # Invoke RAG pipeline to get answer\n",
    "    answer = rag_chain.invoke(question)\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Generate answers for all questions\n",
    "generated_answers = [generate_answers(question) for question in questions]\n",
    "\n",
    "# Create a new dataset with generated answers\n",
    "generated_data = {\n",
    "    'question': questions,\n",
    "    'answer': generated_answers,\n",
    "    'ground_truth': correct_answers,\n",
    "    'contexts': contexts  # Thêm cột contexts\n",
    "}\n",
    "\n",
    "generated_dataset = Dataset.from_dict(generated_data)\n",
    "\n",
    "# Evaluate generated answers using Ragas metrics\n",
    "score = evaluate(generated_dataset, metrics=[faithfulness, answer_correctness], raise_exceptions=False)\n",
    "score.to_pandas().to_csv('res.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
